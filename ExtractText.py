import html2text
import re
import os
from bs4 import BeautifulSoup

from Util import return_files_above_token_count, get_file_paths, get_file_tokens, read_text_from_file, \
    write_tokens_to_file, split_file, return_files_below_token_count


# regex generated by chatgpt

def extract_categories(text):
    category_pattern = re.compile(r'\bPART \w+ â€“ Category \d+|CATEGORY \d+ - [A-Z]+\s|^\d+[A-Z]+\s+[A-Z][a-z]+',
                                  re.MULTILINE)

    # 0A001
    if re.search(category_pattern, text):
        categories = re.split(category_pattern, text)

        if not os.path.isdir('Categories'):
            os.makedirs('Categories')

        for i, category in enumerate(categories[1:], start=1):
            folder_path = './Categories'
            filename = f'{folder_path}/category_{i}.txt'
            if i == (len(categories) - 1):
                pattern = re.compile(r'^ANNEX\s+\w+', re.MULTILINE)
                last_category = re.split(pattern, category)
                category = last_category[0]

            with open(filename, 'w', encoding='utf-8') as file:
                file.write(category.strip())

        print(f"Processed and saved {len(categories) - 1} categories.")


def extract_articles(text):
    article_pattern = re.compile(r'^Article \d+[\s.:]*(?=\n|$)', re.IGNORECASE | re.MULTILINE)

    if re.search(article_pattern, text):
        articles = re.split(article_pattern, text)

        if not os.path.isdir('Articles'):
            os.makedirs('Articles')

        for i, article in enumerate(articles[1:], start=1):
            folder_path = './Articles'
            filename = f'{folder_path}/article_{i}.txt'
            if i == (len(articles) - 1):
                pattern = re.compile(r'^ANNEX\s+\w+', re.MULTILINE)
                last_article = re.split(pattern, article)
                article = last_article[0]

            with open(filename, 'w', encoding='utf-8') as file:
                file.write(article.strip())

        print(f"Processed and saved articles {len(articles) - 1}")


def extract_technical_notes(html):
    soup = BeautifulSoup(html, 'html.parser')
    tech_notes = soup.find_all('p', class_='oj-ti-annotation')

    if not os.path.isdir('TechnicalNotes'):
        os.makedirs('TechnicalNotes')

    for i, tech_note in enumerate(tech_notes):
        desc_texts = []
        sibling = tech_note.find_next_sibling()
        while sibling and sibling.name == 'p' and 'oj-normal' in sibling.get('class', []):
            if sibling.find("span", class_="oj-italic"):
                desc_texts.append(sibling.get_text(strip=True))

            sibling = sibling.find_next_sibling()

        final_text = "\n".join(desc_texts)
        folder_path = './TechnicalNotes'
        filename = f'{folder_path}/technical_note_{i}'
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(final_text.strip())


def extract_annex(text):
    pattern = re.compile(r'^ANNEX\s+\w+', re.MULTILINE)
    if re.search(pattern, text):
        if not os.path.isdir('Annexes'):
            os.makedirs('Annexes')

        annexes = re.split(pattern, text)

        for i, annex in enumerate(annexes[1:]):
            folder_path = './Annexes'
            filename = f'{folder_path}/annex_{i}.txt'

            with open(filename, 'w', encoding='utf-8') as file:
                file.write(annex.strip())


if __name__ == '__main__':
    with open("./L_2021206EN.01000101.xml.html", 'r', encoding='utf-8') as file:
        html = file.read()

    h = html2text.HTML2Text()
    h.ignore_links = True

    h.wrap_tables = True
    h.single_line_break = True
    h.wrap_list_items = True
    h.ignore_images = True
    text = h.handle(html)

    # with open('extractedMD.txt', 'w', encoding='utf-8') as file:
    # file.write(text)

    # extract_categories(text)
    # extract_articles(text)
    # extract_annex(text)
    # extract_technical_notes(html)

    text_files_paths = get_file_paths('./ProcessedData')
    files_and_tokens = get_file_tokens(text_files_paths)
    print(*files_and_tokens, sep='\n')
    # write_tokens_to_file(files_and_tokens, 'tokens')
    # files_above_token_max = return_files_above_token_count(files_and_tokens, 500)
    # # files_below_token_max = return_files_below_token_count(files_and_tokens,500)
    # print(*files_above_token_max, sep='\n')

    # if not os.path.isdir('ProcessedData'):
    #     os.makedirs('ProcessedData')
    #
    # for fileIndex, file in enumerate(files_above_token_max):
    #     chunks = split_file(file[0], 500)
    #
    #     for i, chunk in enumerate(chunks):
    #         filename = f'ProcessedData/file_{fileIndex}_chunk_{i}.txt'
    #         with open(filename, 'w', encoding='utf-8') as f:
    #             f.write(chunk)
